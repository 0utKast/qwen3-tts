
********
Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.
********
 
<class 'module'>
Help on module qwen_tts.inference.qwen3_tts_model in qwen_tts.inference:

NAME
    qwen_tts.inference.qwen3_tts_model

DESCRIPTION
    # coding=utf-8
    # Copyright 2026 The Alibaba Qwen team.
    # SPDX-License-Identifier: Apache-2.0
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

CLASSES
    builtins.object
        Qwen3TTSModel
        VoiceClonePromptItem

    class Qwen3TTSModel(builtins.object)
     |  Qwen3TTSModel(
     |      model: qwen_tts.core.models.modeling_qwen3_tts.Qwen3TTSForConditionalGeneration,
     |      processor,
     |      generate_defaults: Optional[Dict[str, Any]] = None
     |  )
     |
     |  A HuggingFace-style wrapper for Qwen3 TTS models (CustomVoice/VoiceDesign/Base) that provides:
     |    - from_pretrained() initialization via AutoModel/AutoProcessor
     |    - generation APIs for:
     |        * CustomVoice: generate_custom_voice()
     |        * VoiceDesign: generate_voice_design()
     |        * Base: generate_voice_clone() + create_voice_clone_prompt()
     |    - consistent output: (wavs: List[np.ndarray], sample_rate: int)
     |
     |  Notes:
     |    - This wrapper expects the underlying model class to be `Qwen3TTSForConditionalGeneration`
     |    - Language / speaker validation is done via model methods:
     |        model.get_supported_languages(), model.get_supported_speakers()
     |
     |  Methods defined here:
     |
     |  __init__(
     |      self,
     |      model: qwen_tts.core.models.modeling_qwen3_tts.Qwen3TTSForConditionalGeneration,
     |      processor,
     |      generate_defaults: Optional[Dict[str, Any]] = None
     |  )
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  create_voice_clone_prompt(
     |      self,
     |      ref_audio: Union[str, numpy.ndarray, Tuple[numpy.ndarray, int], List[Union[str, numpy.ndarray, Tuple[numpy.ndarray, int]]]],
     |      ref_text: Union[str, List[Optional[str]], NoneType] = None,
     |      x_vector_only_mode: Union[bool, List[bool]] = False
     |  ) -> List[qwen_tts.inference.qwen3_tts_model.VoiceClonePromptItem]
     |      Build voice-clone prompt items from reference audio (and optionally reference text) using Base model.
     |
     |      Modes:
     |        - x_vector_only_mode=True:
     |            Only speaker embedding is used to clone voice; ref_text/ref_code are ignored.
     |            This is mutually exclusive with ICL.
     |        - x_vector_only_mode=False:
     |            ICL mode is enabled automatically (icl_mode=True). In this case ref_text is required,
     |            because the model continues/conditions on the reference text + reference speech codes.
     |
     |      Batch behavior:
     |        - ref_audio can be a single item or a list.
     |        - ref_text and x_vector_only_mode can be scalars or lists.
     |        - If any of them are lists with length > 1, lengths must match.
     |
     |      Audio input:
     |        - str: local wav path / URL / base64
     |        - (np.ndarray, sr): waveform + sampling rate
     |
     |      Args:
     |          ref_audio:
     |              Reference audio(s) used to extract:
     |                - ref_code via `model.speech_tokenizer.encode(...)`
     |                - ref_spk_embedding via `model.extract_speaker_embedding(...)` (resampled to 24k)
     |          ref_text:
     |              Reference transcript(s). Required when x_vector_only_mode=False (ICL mode).
     |          x_vector_only_mode:
     |              Whether to use speaker embedding only. If False, ICL mode will be used.
     |
     |      Returns:
     |          List[VoiceClonePromptItem]:
     |              List of prompt items that can be converted into `voice_clone_prompt` dict.
     |
     |      Raises:
     |          ValueError:
     |              - If x_vector_only_mode=False but ref_text is missing.
     |              - If batch lengths mismatch.
     |
     |  generate_custom_voice(
     |      self,
     |      text: Union[str, List[str]],
     |      speaker: Union[str, List[str]],
     |      language: Union[str, List[str]] = None,
     |      instruct: Union[str, List[str], NoneType] = None,
     |      non_streaming_mode: bool = True,
     |      **kwargs
     |  ) -> Tuple[List[numpy.ndarray], int]
     |      Generate speech with the CustomVoice model using a predefined speaker id, optionally controlled by instruction text.
     |
     |      Args:
     |          text:
     |              Text(s) to synthesize.
     |          language:
     |              Language(s) for each sample.
     |          speaker:
     |              Speaker name(s). Will be validated against `model.get_supported_speakers()` (case-insensitive).
     |          instruct:
     |              Optional instruction(s). If None, treated as empty (no instruction).
     |          non_streaming_mode:
     |              Using non-streaming text input, this option currently only simulates streaming text input when set to `false`,
     |              rather than enabling true streaming input or streaming generation.
     |          do_sample:
     |              Whether to use sampling, recommended to be set to `true` for most use cases.
     |          top_k:
     |              Top-k sampling parameter.
     |          top_p:
     |              Top-p sampling parameter.
     |          temperature:
     |              Sampling temperature; higher => more random.
     |          repetition_penalty:
     |              Penalty to reduce repeated tokens/codes.
     |          subtalker_dosample:
     |              Sampling switch for the sub-talker (only valid for qwen3-tts-tokenizer-v2) if applicable.
     |          subtalker_top_k:
     |              Top-k for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          subtalker_top_p:
     |              Top-p for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          subtalker_temperature:
     |              Temperature for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          max_new_tokens:
     |              Maximum number of new codec tokens to generate.
     |          **kwargs:
     |              Any other keyword arguments supported by HuggingFace Transformers `generate()` can be passed.
     |              They will be forwarded to the underlying `Qwen3TTSForConditionalGeneration.generate(...)`.
     |
     |      Returns:
     |          Tuple[List[np.ndarray], int]:
     |              (wavs, sample_rate)
     |
     |      Raises:
     |          ValueError:
     |              If any speaker/language is unsupported or batch sizes mismatch.
     |
     |  generate_voice_clone(
     |      self,
     |      text: Union[str, List[str]],
     |      language: Union[str, List[str]] = None,
     |      ref_audio: Union[str, numpy.ndarray, Tuple[numpy.ndarray, int], List[Union[str, numpy.ndarray, Tuple[numpy.ndarray, int]]], NoneType] = None,
     |      ref_text: Union[str, List[Optional[str]], NoneType] = None,
     |      x_vector_only_mode: Union[bool, List[bool]] = False,
     |      voice_clone_prompt: Union[Dict[str, Any], List[qwen_tts.inference.qwen3_tts_model.VoiceClonePromptItem], NoneType] = None,
     |      non_streaming_mode: bool = False,
     |      **kwargs
     |  ) -> Tuple[List[numpy.ndarray], int]
     |      Voice clone speech using the Base model.
     |
     |      You can provide either:
     |        - (ref_audio, ref_text, x_vector_only_mode) and let this method build the prompt, OR
     |        - `VoiceClonePromptItem` returned by `create_voice_clone_prompt`, OR
     |        - a list of `VoiceClonePromptItem` returned by `create_voice_clone_prompt`.
     |
     |      `ref_audio` Supported forms:
     |      - str: wav path / URL / base64 audio string
     |      - (np.ndarray, sr): waveform + sampling rate
     |      - list of the above
     |
     |      Input flexibility:
     |        - text/language can be scalar or list.
     |        - prompt can be single or batch.
     |        - If batch mode (len(text)>1), lengths must match.
     |
     |      Args:
     |          text:
     |              Text(s) to synthesize.
     |          language:
     |              Language(s) for each sample.
     |          ref_audio:
     |              Reference audio(s) for prompt building. Required if voice_clone_prompt is not provided.
     |          ref_text:
     |              Reference text(s) used for ICL mode (required when x_vector_only_mode=False).
     |          x_vector_only_mode:
     |              If True, only speaker embedding is used (ignores ref_text/ref_code).
     |              If False, ICL mode is used automatically.
     |          voice_clone_prompt:
     |              list[VoiceClonePromptItem] from `create_voice_clone_prompt`.
     |          non_streaming_mode:
     |              Using non-streaming text input, this option currently only simulates streaming text input when set to `false`,
     |              rather than enabling true streaming input or streaming generation.
     |          do_sample:
     |              Whether to use sampling, recommended to be set to `true` for most use cases.
     |          top_k:
     |              Top-k sampling parameter.
     |          top_p:
     |              Top-p sampling parameter.
     |          temperature:
     |              Sampling temperature; higher => more random.
     |          repetition_penalty:
     |              Penalty to reduce repeated tokens/codes.
     |          subtalker_dosample:
     |              Sampling switch for the sub-talker (only valid for qwen3-tts-tokenizer-v2) if applicable.
     |          subtalker_top_k:
     |              Top-k for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          subtalker_top_p:
     |              Top-p for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          subtalker_temperature:
     |              Temperature for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          max_new_tokens:
     |              Maximum number of new codec tokens to generate.
     |          **kwargs:
     |              Any other keyword arguments supported by HuggingFace Transformers `generate()` can be passed.
     |              They will be forwarded to the underlying `Qwen3TTSForConditionalGeneration.generate(...)`.
     |
     |      Returns:
     |          Tuple[List[np.ndarray], int]:
     |              (wavs, sample_rate)
     |
     |      Raises:
     |          ValueError:
     |              If batch sizes mismatch or required prompt inputs are missing.
     |
     |  generate_voice_design(
     |      self,
     |      text: Union[str, List[str]],
     |      instruct: Union[str, List[str]],
     |      language: Union[str, List[str]] = None,
     |      non_streaming_mode: bool = True,
     |      **kwargs
     |  ) -> Tuple[List[numpy.ndarray], int]
     |      Generate speech with the VoiceDesign model using natural-language style instructions.
     |
     |      Args:
     |          text:
     |              Text(s) to synthesize.
     |          language:
     |              Language(s) for each sample.
     |          instruct:
     |              Instruction(s) describing desired voice/style. Empty string is allowed (treated as no instruction).
     |          non_streaming_mode:
     |              Using non-streaming text input, this option currently only simulates streaming text input when set to `false`,
     |              rather than enabling true streaming input or streaming generation.
     |          do_sample:
     |              Whether to use sampling, recommended to be set to `true` for most use cases.
     |          top_k:
     |              Top-k sampling parameter.
     |          top_p:
     |              Top-p sampling parameter.
     |          temperature:
     |              Sampling temperature; higher => more random.
     |          repetition_penalty:
     |              Penalty to reduce repeated tokens/codes.
     |          subtalker_dosample:
     |              Sampling switch for the sub-talker (only valid for qwen3-tts-tokenizer-v2) if applicable.
     |          subtalker_top_k:
     |              Top-k for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          subtalker_top_p:
     |              Top-p for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          subtalker_temperature:
     |              Temperature for sub-talker sampling (only valid for qwen3-tts-tokenizer-v2).
     |          max_new_tokens:
     |              Maximum number of new codec tokens to generate.
     |          **kwargs:
     |              Any other keyword arguments supported by HuggingFace Transformers `generate()` can be passed.
     |              They will be forwarded to the underlying `Qwen3TTSForConditionalGeneration.generate(...)`.
     |
     |      Returns:
     |          Tuple[List[np.ndarray], int]:
     |              (wavs, sample_rate)
     |
     |  get_supported_languages(self) -> Optional[List[str]]
     |      List supported language names for the current model.
     |
     |      This is a convenience wrapper around `model.get_supported_languages()`.
     |      If the underlying model does not expose language constraints (returns None),
     |      this method also returns None.
     |
     |      Returns:
     |          Optional[List[str]]:
     |              - A sorted list of supported language names (lowercased), if available.
     |              - None if the model does not provide supported languages.
     |
     |  get_supported_speakers(self) -> Optional[List[str]]
     |      List supported speaker names for the current model.
     |
     |      This is a convenience wrapper around `model.get_supported_speakers()`.
     |      If the underlying model does not expose speaker constraints (returns None),
     |      this method also returns None.
     |
     |      Returns:
     |          Optional[List[str]]:
     |              - A sorted list of supported speaker names (lowercased), if available.
     |              - None if the model does not provide supported speakers.
     |
     |  ----------------------------------------------------------------------
     |  Class methods defined here:
     |
     |  from_pretrained(pretrained_model_name_or_path: str, **kwargs) -> 'Qwen3TTSModel'
     |      Load a Qwen3 TTS model and its processor in HuggingFace `from_pretrained` style.
     |
     |      This method:
     |        1) Loads config via AutoConfig (so your side can register model_type -> config/model).
     |        2) Loads the model via AutoModel.from_pretrained(...), forwarding `kwargs` unchanged.
     |        3) Loads the processor via AutoProcessor.from_pretrained(model_path).
     |        4) Loads optional `generate_config.json` from the model directory/repo snapshot if present.
     |
     |      Args:
     |          pretrained_model_name_or_path (str):
     |              HuggingFace repo id or local directory of the model.
     |          **kwargs:
     |              Forwarded as-is into `AutoModel.from_pretrained(...)`.
     |              Typical examples: device_map="cuda:0", dtype=torch.bfloat16, attn_implementation="flash_attention_2".
     |
     |      Returns:
     |          Qwen3TTSModel:
     |              Wrapper instance containing `model`, `processor`, and generation defaults.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object

    class VoiceClonePromptItem(builtins.object)
     |  VoiceClonePromptItem(
     |      ref_code: Optional[torch.Tensor],
     |      ref_spk_embedding: torch.Tensor,
     |      x_vector_only_mode: bool,
     |      icl_mode: bool,
     |      ref_text: Optional[str] = None
     |  ) -> None
     |
     |  Container for one sample's voice-clone prompt information that can be fed to the model.
     |
     |  Fields are aligned with `Qwen3TTSForConditionalGeneration.generate(..., voice_clone_prompt=...)`.
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(
     |      self,
     |      ref_code: Optional[torch.Tensor],
     |      ref_spk_embedding: torch.Tensor,
     |      x_vector_only_mode: bool,
     |      icl_mode: bool,
     |      ref_text: Optional[str] = None
     |  ) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __replace__ = _replace(self, /, **changes) from dataclasses
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {'icl_mode': <class 'bool'>, 'ref_code': typing.Opti...
     |
     |  __dataclass_fields__ = {'icl_mode': Field(name='icl_mode',type=<class ...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('ref_code', 'ref_spk_embedding', 'x_vector_only_mode...
     |
     |  ref_text = None

DATA
    AudioLike = typing.Union[str, numpy.ndarray, typing.Tuple[numpy.ndarra...
    Dict = typing.Dict
        A generic version of dict.

    List = typing.List
        A generic version of list.

    MaybeList = typing.Union[typing.Any, typing.List[typing.Any]]
    Optional = typing.Optional
        Optional[X] is equivalent to Union[X, None].

    Tuple = typing.Tuple
        Deprecated alias to builtins.tuple.

        Tuple[X, Y] is the cross-product type of X and Y.

        Example: Tuple[T1, T2] is a tuple of two elements corresponding
        to type variables T1 and T2.  Tuple[int, float, str] is a tuple
        of an int, a float and a string.

        To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].

    Union = typing.Union
        Union type; Union[X, Y] means either X or Y.

        On Python 3.10 and higher, the | operator
        can also be used to denote unions;
        X | Y means the same thing to the type checker as Union[X, Y].

        To define a union, use e.g. Union[int, str]. Details:
        - The arguments must be types and there must be at least one.
        - None as an argument is a special case and is replaced by
          type(None).
        - Unions of unions are flattened, e.g.::

            assert Union[Union[int, str], float] == Union[int, str, float]

        - Unions of a single argument vanish, e.g.::

            assert Union[int] == int  # The constructor actually returns int

        - Redundant arguments are skipped, e.g.::

            assert Union[int, str, int] == Union[int, str]

        - When comparing unions, the argument order is ignored, e.g.::

            assert Union[int, str] == Union[str, int]

        - You cannot subclass or instantiate a union.
        - You can use Optional[X] as a shorthand for Union[X, None].

FILE
    c:\users\educa\appdata\roaming\python\python313\site-packages\qwen_tts\inference\qwen3_tts_model.py


None
